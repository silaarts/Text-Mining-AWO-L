{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc53118b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45063123",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: pip in /opt/homebrew/lib/python3.9/site-packages (22.0.4)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.9/site-packages (1.10.2)\n",
      "Collecting torch\n",
      "  Using cached torch-1.11.0-cp39-none-macosx_11_0_arm64.whl (43.1 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.12.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-0.11.0-cp39-cp39-macosx_11_0_arm64.whl (2.3 MB)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.9/site-packages (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.9/site-packages (1.22.3)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.9/site-packages (4.17.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/homebrew/lib/python3.9/site-packages (7.7.0)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.5.1-cp39-cp39-macosx_11_0_arm64.whl (7.2 MB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.9/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.2-cp39-cp39-macosx_12_0_arm64.whl (6.9 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: sacremoses in /opt/homebrew/lib/python3.9/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (4.63.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/homebrew/lib/python3.9/site-packages (from ipywidgets) (6.10.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/homebrew/lib/python3.9/site-packages (from ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/homebrew/lib/python3.9/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/homebrew/lib/python3.9/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/homebrew/lib/python3.9/site-packages (from ipywidgets) (8.2.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/homebrew/lib/python3.9/site-packages (from ipywidgets) (5.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/homebrew/lib/python3.9/site-packages (from ipywidgets) (5.1.1)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.2-cp39-cp39-macosx_11_0_arm64.whl (63 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.31.2-py3-none-any.whl (899 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (3.0.7)\n",
      "Collecting scipy>=1.0\n",
      "  Using cached scipy-1.8.0-cp39-cp39-macosx_12_0_arm64.whl (28.7 MB)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/homebrew/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: appnope in /opt/homebrew/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: nest-asyncio in /opt/homebrew/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/homebrew/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.2.0)\n",
      "Requirement already satisfied: tornado<7.0,>=5.0 in /opt/homebrew/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/homebrew/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: pickleshare in /opt/homebrew/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /opt/homebrew/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/homebrew/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: backcall in /opt/homebrew/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/homebrew/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.28)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/homebrew/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (61.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/homebrew/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/homebrew/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/homebrew/lib/python3.9/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/homebrew/lib/python3.9/site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/homebrew/lib/python3.9/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/homebrew/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/homebrew/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: entrypoints in /opt/homebrew/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/homebrew/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/homebrew/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/homebrew/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/homebrew/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/homebrew/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.3)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/homebrew/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.4.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/lib/python3.9/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: asttokens in /opt/homebrew/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/homebrew/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing in /opt/homebrew/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/homebrew/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/homebrew/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/homebrew/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.13)\n",
      "Requirement already satisfied: testpath in /opt/homebrew/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/homebrew/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.10.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/homebrew/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/homebrew/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/homebrew/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/homebrew/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /opt/homebrew/lib/python3.9/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
      "Installing collected packages: torch, scipy, kiwisolver, fonttools, cycler, torchvision, torchaudio, scikit-learn, matplotlib, sklearn, seaborn\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.2\n",
      "    Uninstalling torch-1.10.2:\n",
      "      Successfully uninstalled torch-1.10.2\n",
      "\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0mSuccessfully installed cycler-0.11.0 fonttools-4.31.2 kiwisolver-1.4.2 matplotlib-3.5.1 scikit-learn-1.0.2 scipy-1.8.0 seaborn-0.11.2 sklearn-0.0 torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip torch torchvision torchaudio pandas numpy sklearn transformers ipywidgets matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0089c3d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from preprocess_indexqual import load_html_transcripts\n",
    "from preprocess_sentiment import preprocess_sent\n",
    "from preprocess_sentiment_test import preprocess_sentiment_test\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af08057",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd472c0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe862bb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a6c587",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f52b5f1e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "# use CUDA when available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using:\", device)\n",
    "\n",
    "# clean\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# define max length\n",
    "max_length = 512\n",
    "\n",
    "def model_init():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = RobertaForSequenceClassification.from_pretrained('pdelobelle/robbert-v2-dutch-base', num_labels=len(classes))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f35f8b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa9bb196",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to train BERT with sentiment data\n",
      "preprocessing ...\n",
      "                                                text  negative  neutral  \\\n",
      "0  Weinig activiteitenbegeleiding voor zo een men...         1        0   \n",
      "1                                   Nee dat is mooi.         0        0   \n",
      "2  Je daar heeft ze het toch ook wel ze spreekt d...         1        0   \n",
      "3                             Dat moet je niet doen.         1        0   \n",
      "4                             Begon ze te vertellen.         0        0   \n",
      "\n",
      "   positive  \n",
      "0         0  \n",
      "1         1  \n",
      "2         0  \n",
      "3         0  \n",
      "4         1  \n",
      "Label columns:  ['negative', 'neutral', 'positive']\n",
      "5861\n"
     ]
    }
   ],
   "source": [
    "# print start info\n",
    "print(\"starting to train BERT with sentiment data\")\n",
    "print(\"preprocessing ...\")\n",
    "\n",
    "# load preprocessed data\n",
    "df = preprocess_sent()\n",
    "\n",
    "# to csv\n",
    "df.to_csv('preprocessed_sentiment.csv', index = False, header=True)\n",
    "    \n",
    "# show data\n",
    "print(df.head())\n",
    "    \n",
    "# select label columns\n",
    "cols = df.columns\n",
    "label_cols = list(cols[1:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)\n",
    "classes = label_cols\n",
    "\n",
    "# set header for all label columns\n",
    "df['labels'] = list(df[label_cols].values)\n",
    "df.head()\n",
    "print(len(df))\n",
    "\n",
    "# get input and outputs\n",
    "labels = list(df.labels.values)\n",
    "sentences = list(df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c85fd003",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokenizer = RobertaTokenizer.from_pretrained('pdelobelle/robbert-v2-dutch-base')  # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(sentences, truncation=True,\n",
    "                                    max_length=max_length,\n",
    "                                    padding=True)\n",
    "print('tokenizer outputs: ', encodings.keys())\n",
    "\n",
    "# preparing data format for training\n",
    "input_ids = encodings['input_ids']  # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask']  # attention \n",
    "\n",
    "# Use train_test_split to split our data into train and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks, random_state=2020, test_size=0.10, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83dd1e82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/fggrmb4x10d0v5w9y2fc9x_h0000gn/T/ipykernel_72026/776644167.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  train_labels = torch.tensor(train_labels)\n"
     ]
    }
   ],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33b2216",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48,\n",
    "# or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because,\n",
    "# unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_labels)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_labels)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a241cdf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47883e5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pdelobelle/robbert-v2-dutch-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "# TODO add seed constant\n",
    "model = model_init()\n",
    "model.to(device)\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "train_loss_per_epoch = []\n",
    "valid_loss_per_epoch = []\n",
    "valid_acc_set = []\n",
    "best_valid_f1 = 0\n",
    "best_name = \"\"\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c28f00",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    best_name = 'bert_model_sentiment_' + str(val_f1_accuracy)\n",
    "    dic = zip(range(0, len(classes)), classes)\n",
    "    torch.save(model.state_dict(), best_name)\n",
    "    \n",
    "    return best_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32519b01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [05:44<00:00,  2.09s/it]\n",
      "100%|██████████| 165/165 [05:44<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5573261168870058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:11<00:00,  1.71it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5102152102871945\n",
      "F1 Validation Accuracy:  56.63716814159292\n",
      "Flat Validation Accuracy:  48.040885860306645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [05:42<00:00,  2.08s/it]/it]\n",
      "100%|██████████| 165/165 [05:42<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4610886590047316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:11<00:00,  1.69it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.49627655587698283\n",
      "F1 Validation Accuracy:  62.26583407671722\n",
      "Flat Validation Accuracy:  57.5809199318569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [11:49<00:00, 354.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0  # running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True) as pbar:\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for multilabel classification\n",
    "            logits = model(b_input_ids, b_input_mask)\n",
    "            loss_func = BCEWithLogitsLoss()\n",
    "\n",
    "            loss = loss_func(logits[0].view(-1, num_labels),\n",
    "                             b_labels.type_as(logits[0]).view(-1, num_labels))  # convert labels to float for calculation\n",
    "            train_loss_set.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
    "    train_loss_per_epoch.append(tr_loss / nb_tr_steps)\n",
    "    \n",
    "    ###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds, true_labels, pred_labels, tokenized_texts = [], [], [], []\n",
    "\n",
    "    # Tracking variables\n",
    "    vd_loss = 0  # running loss\n",
    "    nb_vd_steps = 0\n",
    "    \n",
    "    # Predict\n",
    "    with tqdm(total=len(validation_dataloader), position=0, leave=True) as pbar:\n",
    "        for i, batch in enumerate(tqdm(validation_dataloader, position=0, leave=True)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                b_logit_pred = model(b_input_ids, b_input_mask)\n",
    "\n",
    "                loss = loss_func(b_logit_pred[0].view(-1, num_labels),\n",
    "                     b_labels.type_as(b_logit_pred[0]).view(-1, num_labels))  # convert labels to float for calculation\n",
    "                vd_loss += loss.item()\n",
    "                nb_vd_steps += 1\n",
    "\n",
    "                pred_label = torch.sigmoid(b_logit_pred[0])\n",
    "                b_logit_pred = b_logit_pred[0].detach().cpu().numpy()\n",
    "                pred_label = pred_label.to('cpu').numpy()\n",
    "                b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tokenized_texts.append(b_input_ids)\n",
    "            logit_preds.append(b_logit_pred)\n",
    "            true_labels.append(b_labels)\n",
    "            pred_labels.append(pred_label)\n",
    "            \n",
    "            pbar.update()\n",
    "    \n",
    "    print(\"Validation loss: {}\".format(vd_loss / nb_vd_steps))\n",
    "    valid_loss_per_epoch.append(vd_loss / nb_vd_steps)\n",
    "        \n",
    "    # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl > threshold for pl in pred_labels]\n",
    "    true_bools = [tl == 1 for tl in true_labels]\n",
    "    val_f1_accuracy = f1_score(true_bools, pred_bools, average='micro') * 100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools) * 100\n",
    "\n",
    "    valid_acc_set.append(val_f1_accuracy)\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)   \n",
    "    \n",
    "    if val_f1_accuracy > best_valid_f1:\n",
    "        if os.path.exists(best_name) and os.path.isdir(best_name):\n",
    "            shutil.rmtree(best_name)\n",
    "        best_valid_f1 = val_f1_accuracy\n",
    "        best_name = save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b4c9c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Simple predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6434eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb17605",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe723516",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "codes, test_df = preprocess_sentiment_test()\n",
    "\n",
    "# select label columns\n",
    "cols = test_df.columns\n",
    "label_cols = list(cols[1:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)\n",
    "classes = label_cols\n",
    "\n",
    "# set header for all label columns\n",
    "test_df['labels'] = list(test_df[label_cols].values)\n",
    "\n",
    "# Gathering input data\n",
    "test_labels = list(test_df.labels.values)\n",
    "test_comments = list(test_df.sentence.values)\n",
    "\n",
    "\n",
    "# Encoding input data\n",
    "test_encodings = tokenizer.batch_encode_plus(test_comments,max_length=max_length,pad_to_max_length=True)\n",
    "test_input_ids = test_encodings['input_ids']\n",
    "test_attention_masks = test_encodings['attention_mask']\n",
    "\n",
    "# Make tensors out of data\n",
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "test_attention_masks = torch.tensor(test_attention_masks)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_labels)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "# Variables to gather full output\n",
    "logit_preds, true_labels, pred_labels, tokenized_texts = [], [], [], []\n",
    "\n",
    "# Use original distribution for evaluation (instead of a balanced distribution)\n",
    "# validation_sampler = SequentialSampler(validation_labels)\n",
    "# validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# Predict\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        b_logit_pred = model(b_input_ids, b_input_mask)\n",
    "        pred_label = torch.sigmoid(b_logit_pred[0])\n",
    "\n",
    "        b_logit_pred = b_logit_pred[0].detach().cpu().numpy()\n",
    "        pred_label = pred_label.to('cpu').numpy()\n",
    "        b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tokenized_texts.append(b_input_ids)\n",
    "    logit_preds.append(b_logit_pred)\n",
    "    true_labels.append(b_labels)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "# Flatten outputs\n",
    "true_labels = [item for sublist in true_labels for item in sublist]\n",
    "pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "\n",
    "# Calculate Accuracy\n",
    "threshold = 0.50\n",
    "pred_bools = [pl > threshold for pl in pred_labels]\n",
    "true_bools = [tl == 1 for tl in true_labels]\n",
    "val_f1_accuracy = f1_score(true_bools, pred_bools, average='micro') * 100\n",
    "val_flat_accuracy = accuracy_score(true_bools, pred_bools) * 100\n",
    "\n",
    "print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "print('Flat Validation Accuracy: ', val_flat_accuracy)   \n",
    "\n",
    "# calculate predicted class for single-label CFM\n",
    "true_labels_single = np.argmax(true_labels, axis=1)\n",
    "pred_labels_single = np.argmax(pred_labels, axis=1)\n",
    "\n",
    "cm = confusion_matrix(true_labels_single, pred_labels_single)\n",
    "cm_rot = np.fliplr(np.rot90(cm))\n",
    "\n",
    "cm_rot_df = pd.DataFrame(cm_rot, index=classes[::-1], columns=classes[::-1])\n",
    "cm_rot_df.index.name = 'As predicted by text mining'\n",
    "cm_rot_df.columns.name = 'Manually coded'\n",
    "\n",
    "print(cm_rot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18614458",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "figsize = (8, 8)\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(train_loss_per_epoch, 'g')\n",
    "# plt.plot(valid_loss_per_epoch, 'b')\n",
    "# plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.heatmap(cm_rot_df, annot=cm_rot, fmt='', cmap=\"Blues\", annot_kws={\"size\": 20}, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a39a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load transcripts\n",
    "_, transcripts, filenames = load_html_transcripts()\n",
    "\n",
    "for j in range(0, len(transcripts)):\n",
    "    \n",
    "    if '.html' not in filenames[j]:\n",
    "        continue\n",
    "    \n",
    "    context_text = ''\n",
    "\n",
    "    obj_list = []\n",
    "    \n",
    "    lines = transcripts[j]\n",
    "    \n",
    "    for i in range(0, len(lines)):\n",
    "        sentence = tokenizer(lines[i], truncation=True,\n",
    "                                        max_length=max_length,\n",
    "                                        padding=True, return_tensors='pt')\n",
    "\n",
    "        sentence.to(device)\n",
    "\n",
    "        result = model(sentence.input_ids, sentence.attention_mask)\n",
    "\n",
    "        pred_labels = result.logits.detach().cpu().numpy()\n",
    "        pred_labels_single = np.argmax(pred_labels, axis=1)\n",
    "        \n",
    "        obj = {\n",
    "            \"text\": lines[i],\n",
    "            \"label\": int(pred_labels_single[0])\n",
    "        }\n",
    "\n",
    "        obj_list.append(obj)\n",
    "\n",
    "        \n",
    "    pos = [segment for segment in obj_list if segment['label'] == 2]\n",
    "    neg = [segment for segment in obj_list if segment['label'] == 0]\n",
    "    pos_len = len(pos)\n",
    "    neg_len = len(neg)\n",
    "        \n",
    "    print(filenames[j], \":\", str(round(pos_len / (pos_len + neg_len) * 100, 1)))\n",
    "        \n",
    "    obj = {\n",
    "        \"filename\": filenames[j],\n",
    "        \"segments\": obj_list\n",
    "    }\n",
    "            \n",
    "    json_dump = json.dumps(obj)\n",
    "\n",
    "    with open(\"output/\" + str(filenames[j]) + \".json\", \"w\") as f:\n",
    "        f.write(json_dump)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}